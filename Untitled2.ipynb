{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea4c372b-8b16-4692-8c65-46a62780b016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d862a31d-2aa5-44da-a670-9f93d3a58aca",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m docs = text_splitter.split_documents(docs)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# í…ìŠ¤íŠ¸ -> ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ì„ë² ë”© ëª¨ë¸ ìƒì„±\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m embeddings = \u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# FAISS ë²¡í„°ì €ì¥ì†Œ ë§Œë“¤ê¸°\u001b[39;00m\n\u001b[32m     13\u001b[39m vectorstore = FAISS.from_documents(docs, embeddings)\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:327\u001b[39m, in \u001b[36mOpenAIEmbeddings.validate_environment\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    325\u001b[39m         \u001b[38;5;28mself\u001b[39m.http_client = httpx.Client(proxy=\u001b[38;5;28mself\u001b[39m.openai_proxy)\n\u001b[32m    326\u001b[39m     sync_specific = {\u001b[33m\"\u001b[39m\u001b[33mhttp_client\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.http_client}\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28mself\u001b[39m.client = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msync_specific\u001b[49m\u001b[43m)\u001b[49m.embeddings  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_client:\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.openai_proxy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.http_async_client:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\_client.py:126\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    124\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    127\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m     )\n\u001b[32m    129\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ìª¼ê°œê¸°\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# í…ìŠ¤íŠ¸ -> ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ì„ë² ë”© ëª¨ë¸ ìƒì„±\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# FAISS ë²¡í„°ì €ì¥ì†Œ ë§Œë“¤ê¸°\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fee8498-709e-4920-a505-3a9d3cf9d183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec3cbc54-337e-41ce-8d50-3e26bfb1922a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26462073-645a-4479-abdc-f841ad96bd3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (1390612049.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mC:\\Users\\admin\u001b[39m\n       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "C:\\Users\\admin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42c5218c-18d8-41c3-8d2d-9099e5c0ebcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "print(os.getenv(\"OPENAI_API_KEY\") is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ef0c9e1-670a-4189-83ca-071295ce4447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "print(os.getenv(\"OPENAI_API_KEY\") is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85ec9543-8282-447a-93ce-a04eef97019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# ë©”ëª¨ì¥ì— ì €ì¥í•œ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œë¥¼ ì •í™•íˆ ì§€ì •í•´ì¤˜\n",
    "loader = TextLoader(\"C:/Users/admin/.env\", encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "\n",
    "# ë¬¸ì„œë¥¼ ì˜ê²Œ ìª¼ê°œê¸°\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# í…ìŠ¤íŠ¸ -> ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ì„ë² ë”© ëª¨ë¸ ìƒì„±\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# FAISS ë²¡í„°ì €ì¥ì†Œ ë§Œë“¤ê¸°\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdd8a941-657f-4b3b-924a-6f72344597a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d138418-378d-4fc3-85ae-2006347340a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_community.vectorstores.faiss.FAISS'>\n"
     ]
    }
   ],
   "source": [
    "print(type(vectorstore))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a557cdfe-9045-430d-80c8-0ef323f1caee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_20400\\141266844.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# ì´ì „ ëŒ€í™” ê¸°ì–µí•  ìˆ˜ ìˆëŠ” ë©”ëª¨ë¦¬ ìƒì„±\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# RAG ì²´ì¸ êµ¬ì„±\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    ChatOpenAI(),                  # OpenAI LLM\n",
    "    vectorstore.as_retriever(),   # FAISS ê¸°ë°˜ ê²€ìƒ‰ê¸°\n",
    "    memory=memory,                # ëŒ€í™” ë©”ëª¨ë¦¬\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89d65e71-a3c0-45d7-965a-e4d2d90d0d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_20400\\49376575.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"question\": question})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì£„ì†¡í•©ë‹ˆë‹¤. LangChainì— ëŒ€í•´ ì•Œë ¤ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "question = \"LangChainì´ ë­ì•¼?\"\n",
    "result = qa_chain({\"question\": question})\n",
    "print(result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1798144f-300d-404d-bd9d-e96bcb4a5ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] OPENAI_API_KEY=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\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = vectorstore.similarity_search(\"LangChainì´ ë­ì•¼?\")\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"[{i}] {r.page_content[:300]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b720b5c5-c9ca-46f5-9a30-c6ad2ce05d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'API í‚¤ëŠ” ë³´ì•ˆ ë° í™•ì¸ ëª©ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. API í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìê°€ ë°ì´í„°ì— ì—‘ì„¸ìŠ¤í•˜ê³  ì •ë³´ì— ëŒ€í•œ ìš”ì²­ì„ '\n",
      "           'ë³´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ë°ì´í„°ì— ëŒ€í•œ ì•¡ì„¸ìŠ¤ë¥¼ ì œí•œí•˜ê³  ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. APIë¥¼ í˜¸ì¶œí•  ë•Œ API í‚¤ë¥¼ '\n",
      "           'ì œê³µí•˜ì—¬ ì¸ì¦ ë° ê¶Œí•œ ë¶€ì—¬ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',\n",
      " 'chat_history': [HumanMessage(content='LangChainì´ ë­ì•¼?', additional_kwargs={}, response_metadata={}),\n",
      "                  AIMessage(content='ì£„ì†¡í•©ë‹ˆë‹¤. LangChainì— ëŒ€í•´ ì•Œë ¤ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}),\n",
      "                  HumanMessage(content='ì´ ë¬¸ì„œì—ì„œ API í‚¤ëŠ” ì–´ë””ì— ì‚¬ìš©ëœë‹¤ê³  ë‚˜ì™€ ìˆì–´?', additional_kwargs={}, response_metadata={}),\n",
      "                  AIMessage(content='API í‚¤ëŠ” ë³´ì•ˆ ë° í™•ì¸ ëª©ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. API í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìê°€ ë°ì´í„°ì— ì—‘ì„¸ìŠ¤í•˜ê³  ì •ë³´ì— ëŒ€í•œ ìš”ì²­ì„ ë³´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ë°ì´í„°ì— ëŒ€í•œ ì•¡ì„¸ìŠ¤ë¥¼ ì œí•œí•˜ê³  ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. APIë¥¼ í˜¸ì¶œí•  ë•Œ API í‚¤ë¥¼ ì œê³µí•˜ì—¬ ì¸ì¦ ë° ê¶Œí•œ ë¶€ì—¬ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.', additional_kwargs={}, response_metadata={})],\n",
      " 'question': 'ì´ ë¬¸ì„œì—ì„œ API í‚¤ëŠ” ì–´ë””ì— ì‚¬ìš©ëœë‹¤ê³  ë‚˜ì™€ ìˆì–´?'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "question = \"ì´ ë¬¸ì„œì—ì„œ API í‚¤ëŠ” ì–´ë””ì— ì‚¬ìš©ëœë‹¤ê³  ë‚˜ì™€ ìˆì–´?\"\n",
    "result = qa_chain.invoke({\"question\": question})\n",
    "pprint(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00cb7383-3413-4406-a053-36da9bd7bac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì£„ì†¡í•©ë‹ˆë‹¤. LangChainì— ëŒ€í•´ ì•Œê³  ìˆì§€ë§Œ ë¬¸ì„œë¥¼ ì–´ë–»ê²Œ ë‚˜ëˆ„ëŠ”ì§€ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì •ë³´ëŠ” ê°€ì§€ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. LangChainì€ OpenAIì—ì„œ ê°œë°œí•œ ì–¸ì–´ ì²˜ë¦¬ ëª¨ë¸ì´ë©°, ì´ ëª¨ë¸ì´ ë¬¸ì„œë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ê³  ë¶„í• í•˜ëŠ”ì§€ì— ëŒ€í•œ ì„¸ë¶€ ë‚´ìš©ì„ ì•Œ ìˆ˜ ìˆëŠ” ì •ë³´ëŠ” í˜„ì¬ ì œê°€ ê°€ì§€ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì¶”ê°€ì ì¸ ë¬¸ì˜ë‚˜ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ë‹¤ì‹œ ë¬¸ì˜í•´ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "question = \"LangChainì—ì„œ ë¬¸ì„œë¥¼ ì–´ë–»ê²Œ ë‚˜ëˆ„ëŠ”ì§€ ì•Œë ¤ì¤˜\"\n",
    "result = qa_chain.invoke({\"question\": question})\n",
    "print(result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99fcad4b-c074-4236-ac5d-2c42bbf886da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì£„ì†¡í•©ë‹ˆë‹¤. 'RecursiveCharacterTextSplitter'ì— ëŒ€í•œ ì •ë³´ë‚˜ ì„¤ëª…ì„ ê°€ì§€ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "question = \"ì´ ë¬¸ì„œì—ì„œ ì‚¬ìš©ëœ RecursiveCharacterTextSplitterê°€ ë¬¸ì„œë¥¼ ë‚˜ëˆ„ëŠ” ë°©ì‹ì€?\"\n",
    "result = qa_chain.invoke({\"question\": question})\n",
    "print(result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38431d41-b46c-4181-99ae-e5655ca3aa51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'LangChainì—ì„œ ë¬¸ì„œë¥¼ ë‚˜ëˆ„ëŠ” ë°©ì‹ì€ ë­ì•¼?', 'chat_history': [HumanMessage(content='LangChainì´ ë­ì•¼?', additional_kwargs={}, response_metadata={}), AIMessage(content='ì£„ì†¡í•©ë‹ˆë‹¤. LangChainì— ëŒ€í•´ ì•Œë ¤ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ì´ ë¬¸ì„œì—ì„œ API í‚¤ëŠ” ì–´ë””ì— ì‚¬ìš©ëœë‹¤ê³  ë‚˜ì™€ ìˆì–´?', additional_kwargs={}, response_metadata={}), AIMessage(content='API í‚¤ëŠ” ë³´ì•ˆ ë° í™•ì¸ ëª©ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. API í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìê°€ ë°ì´í„°ì— ì—‘ì„¸ìŠ¤í•˜ê³  ì •ë³´ì— ëŒ€í•œ ìš”ì²­ì„ ë³´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ë°ì´í„°ì— ëŒ€í•œ ì•¡ì„¸ìŠ¤ë¥¼ ì œí•œí•˜ê³  ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. APIë¥¼ í˜¸ì¶œí•  ë•Œ API í‚¤ë¥¼ ì œê³µí•˜ì—¬ ì¸ì¦ ë° ê¶Œí•œ ë¶€ì—¬ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}), HumanMessage(content='LangChainì—ì„œ ë¬¸ì„œë¥¼ ì–´ë–»ê²Œ ë‚˜ëˆ„ëŠ”ì§€ ì•Œë ¤ì¤˜', additional_kwargs={}, response_metadata={}), AIMessage(content='ì£„ì†¡í•©ë‹ˆë‹¤. LangChainì— ëŒ€í•´ ì•Œê³  ìˆì§€ë§Œ ë¬¸ì„œë¥¼ ì–´ë–»ê²Œ ë‚˜ëˆ„ëŠ”ì§€ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì •ë³´ëŠ” ê°€ì§€ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. LangChainì€ OpenAIì—ì„œ ê°œë°œí•œ ì–¸ì–´ ì²˜ë¦¬ ëª¨ë¸ì´ë©°, ì´ ëª¨ë¸ì´ ë¬¸ì„œë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ê³  ë¶„í• í•˜ëŠ”ì§€ì— ëŒ€í•œ ì„¸ë¶€ ë‚´ìš©ì„ ì•Œ ìˆ˜ ìˆëŠ” ì •ë³´ëŠ” í˜„ì¬ ì œê°€ ê°€ì§€ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì¶”ê°€ì ì¸ ë¬¸ì˜ë‚˜ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ë‹¤ì‹œ ë¬¸ì˜í•´ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ì´ ë¬¸ì„œì—ì„œ ì‚¬ìš©ëœ RecursiveCharacterTextSplitterê°€ ë¬¸ì„œë¥¼ ë‚˜ëˆ„ëŠ” ë°©ì‹ì€?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"ì£„ì†¡í•©ë‹ˆë‹¤. 'RecursiveCharacterTextSplitter'ì— ëŒ€í•œ ì •ë³´ë‚˜ ì„¤ëª…ì„ ê°€ì§€ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='LangChainì—ì„œ ë¬¸ì„œë¥¼ ë‚˜ëˆ„ëŠ” ë°©ì‹ì€ ë­ì•¼?', additional_kwargs={}, response_metadata={}), AIMessage(content='ì£„ì†¡í•©ë‹ˆë‹¤, í˜„ì¬ëŠ” LangChainì´ë‚˜ ê·¸ ë¬¸ì„œë¥¼ ëŒ€í•´ ì œê³µí•˜ëŠ” ì„œë¹„ìŠ¤ì— ëŒ€í•´ ì•Œì§€ ëª»í•©ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={})], 'answer': 'ì£„ì†¡í•©ë‹ˆë‹¤, í˜„ì¬ëŠ” LangChainì´ë‚˜ ê·¸ ë¬¸ì„œë¥¼ ëŒ€í•´ ì œê³µí•˜ëŠ” ì„œë¹„ìŠ¤ì— ëŒ€í•´ ì•Œì§€ ëª»í•©ë‹ˆë‹¤.'}\n"
     ]
    }
   ],
   "source": [
    "question = \"LangChainì—ì„œ ë¬¸ì„œë¥¼ ë‚˜ëˆ„ëŠ” ë°©ì‹ì€ ë­ì•¼?\"\n",
    "result = qa_chain.invoke({\"question\": question})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c676fa85-0258-4ca6-8581-cebea1778c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] OPENAI_API_KEY=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\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vectorstoreì— \"RecursiveCharacterTextSplitter\"ì™€ ê´€ë ¨ëœ ë¬¸ì„œê°€ ìˆëŠ”ì§€ í™•ì¸\n",
    "docs = vectorstore.similarity_search(\"RecursiveCharacterTextSplitter\", k=2)\n",
    "\n",
    "for i, d in enumerate(docs):\n",
    "    print(f\"[{i+1}] {d.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "003de8fc-b6a0-46bc-bf36-7f5cadc0b053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] OPENAI_API_KEY=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\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vectorstoreì— \"RecursiveCharacterTextSplitter\" ê´€ë ¨ ë‚´ìš©ì´ ì˜ ë“¤ì–´ê°”ëŠ”ì§€ ê²€ìƒ‰\n",
    "docs = vectorstore.similarity_search(\"RecursiveCharacterTextSplitter\", k=2)\n",
    "\n",
    "for i, d in enumerate(docs):\n",
    "    print(f\"[{i+1}] {d.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61439444-e403-425a-884f-1b1ff2cd9f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] OPENAI_API_KEY=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\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥ëœ ë¬¸ì„œ ì¤‘ \"RecursiveCharacterTextSplitter\"ì™€ ê´€ë ¨ëœ ë‚´ìš© ê²€ìƒ‰\n",
    "docs = vectorstore.similarity_search(\"RecursiveCharacterTextSplitter\", k=2)\n",
    "\n",
    "for i, d in enumerate(docs):\n",
    "    print(f\"[{i+1}] {d.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fc87c8c-f13e-488e-8683-cde68a8765e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¬¸ì„œ ê°œìˆ˜: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"ë¬¸ì„œ ê°œìˆ˜: {len(docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62e85239-3291-4556-a438-cd797af454c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "OPENAI_API_KEY=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\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs[:5]):\n",
    "    print(f\"[{i+1}]\\n{doc.page_content}\\n{'-'*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dd2fd14-0734-4d7d-ab8a-aa6f2b8a2c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë²¡í„° ì €ì¥ì†Œ ë¬¸ì„œ ìˆ˜: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"ë²¡í„° ì €ì¥ì†Œ ë¬¸ì„œ ìˆ˜: {len(vectorstore.similarity_search('', k=100))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3fe9ba4-f2dc-481b-ab39-6c77b4932042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¬¸ì„œ ê°œìˆ˜: 17\n",
      "[1]\n",
      "Introduction | ğŸ¦œï¸ğŸ”— LangChain\n",
      "--------------------------------------------------------------------------------\n",
      "[2]\n",
      "Skip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1ğŸ’¬SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add\n",
      "--------------------------------------------------------------------------------\n",
      "[3]\n",
      "to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/docs/get_started/introduction\")\n",
    "docs = loader.load()\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"ë¬¸ì„œ ê°œìˆ˜: {len(docs)}\")\n",
    "for i, doc in enumerate(docs[:3]):\n",
    "    print(f\"[{i+1}]\\n{doc.page_content}\\n{'-'*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12d89d1b-ea2c-4b85-b815-cd7ac7a89771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    ChatOpenAI(),\n",
    "    vectorstore.as_retriever(),\n",
    "    memory=memory,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca83e221-4f4c-4dfa-b706-6e11494c9cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'LangChainì—ì„œ ë¬¸ì„œë¥¼ ë‚˜ëˆ„ëŠ” ë°©ì‹ì€ ë­ì•¼?', 'chat_history': [HumanMessage(content='LangChainì—ì„œ ë¬¸ì„œë¥¼ ë‚˜ëˆ„ëŠ” ë°©ì‹ì€ ë­ì•¼?', additional_kwargs={}, response_metadata={}), AIMessage(content='ì£„ì†¡í•©ë‹ˆë‹¤, í˜„ì¬ LangChainì— ëŒ€í•œ ì •ë³´ëŠ” ì—†ìŠµë‹ˆë‹¤.', additional_kwargs={}, response_metadata={})], 'answer': 'ì£„ì†¡í•©ë‹ˆë‹¤, í˜„ì¬ LangChainì— ëŒ€í•œ ì •ë³´ëŠ” ì—†ìŠµë‹ˆë‹¤.'}\n"
     ]
    }
   ],
   "source": [
    "# ì§ˆë¬¸ì„ ë˜ì ¸ë´…ë‹ˆë‹¤.\n",
    "question = \"LangChainì—ì„œ ë¬¸ì„œë¥¼ ë‚˜ëˆ„ëŠ” ë°©ì‹ì€ ë­ì•¼?\"\n",
    "result = qa_chain.invoke({\"question\": question})\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d051bd07-4e88-4e08-aaba-d5cd560d8040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'ì´ ë¬¸ì„œì—ì„œ OPENAI_API_KEYëŠ” ì–´ë–¤ ìš©ë„ë¡œ ì‚¬ìš©ë¼?', 'chat_history': [HumanMessage(content='LangChainì—ì„œ ë¬¸ì„œë¥¼ ë‚˜ëˆ„ëŠ” ë°©ì‹ì€ ë­ì•¼?', additional_kwargs={}, response_metadata={}), AIMessage(content='ì£„ì†¡í•©ë‹ˆë‹¤, í˜„ì¬ LangChainì— ëŒ€í•œ ì •ë³´ëŠ” ì—†ìŠµë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ì´ ë¬¸ì„œì—ì„œ OPENAI_API_KEYëŠ” ì–´ë–¤ ìš©ë„ë¡œ ì‚¬ìš©ë¼?', additional_kwargs={}, response_metadata={}), AIMessage(content='OPENAI_API_KEYëŠ” OpenAIì˜ APIì— ì•¡ì„¸ìŠ¤í•  ë•Œ ì‚¬ìš©ë˜ëŠ” ì¸ì¦ í‚¤ì…ë‹ˆë‹¤. ì´ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ OpenAI APIì— ìš”ì²­ì„ ë³´ë‚´ê³  ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.', additional_kwargs={}, response_metadata={})], 'answer': 'OPENAI_API_KEYëŠ” OpenAIì˜ APIì— ì•¡ì„¸ìŠ¤í•  ë•Œ ì‚¬ìš©ë˜ëŠ” ì¸ì¦ í‚¤ì…ë‹ˆë‹¤. ì´ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ OpenAI APIì— ìš”ì²­ì„ ë³´ë‚´ê³  ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.'}\n"
     ]
    }
   ],
   "source": [
    "question = \"ì´ ë¬¸ì„œì—ì„œ OPENAI_API_KEYëŠ” ì–´ë–¤ ìš©ë„ë¡œ ì‚¬ìš©ë¼?\"\n",
    "result = qa_chain.invoke({\"question\": question})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6338ec0d-a475-4ef4-bf70-4187c48fe6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
