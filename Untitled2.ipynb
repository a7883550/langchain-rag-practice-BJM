{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea4c372b-8b16-4692-8c65-46a62780b016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d862a31d-2aa5-44da-a670-9f93d3a58aca",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m docs = text_splitter.split_documents(docs)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 텍스트 -> 벡터로 변환하는 임베딩 모델 생성\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m embeddings = \u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# FAISS 벡터저장소 만들기\u001b[39;00m\n\u001b[32m     13\u001b[39m vectorstore = FAISS.from_documents(docs, embeddings)\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:327\u001b[39m, in \u001b[36mOpenAIEmbeddings.validate_environment\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    325\u001b[39m         \u001b[38;5;28mself\u001b[39m.http_client = httpx.Client(proxy=\u001b[38;5;28mself\u001b[39m.openai_proxy)\n\u001b[32m    326\u001b[39m     sync_specific = {\u001b[33m\"\u001b[39m\u001b[33mhttp_client\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.http_client}\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28mself\u001b[39m.client = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msync_specific\u001b[49m\u001b[43m)\u001b[49m.embeddings  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_client:\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.openai_proxy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.http_async_client:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\_client.py:126\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    124\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    127\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m     )\n\u001b[32m    129\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 텍스트 쪼개기\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# 텍스트 -> 벡터로 변환하는 임베딩 모델 생성\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# FAISS 벡터저장소 만들기\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fee8498-709e-4920-a505-3a9d3cf9d183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec3cbc54-337e-41ce-8d50-3e26bfb1922a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26462073-645a-4479-abdc-f841ad96bd3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (1390612049.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mC:\\Users\\admin\u001b[39m\n       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "C:\\Users\\admin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42c5218c-18d8-41c3-8d2d-9099e5c0ebcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "print(os.getenv(\"OPENAI_API_KEY\") is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ef0c9e1-670a-4189-83ca-071295ce4447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "print(os.getenv(\"OPENAI_API_KEY\") is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85ec9543-8282-447a-93ce-a04eef97019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 메모장에 저장한 텍스트 파일 경로를 정확히 지정해줘\n",
    "loader = TextLoader(\"C:/Users/admin/.env\", encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 문서를 잘게 쪼개기\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# 텍스트 -> 벡터로 변환하는 임베딩 모델 생성\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# FAISS 벡터저장소 만들기\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdd8a941-657f-4b3b-924a-6f72344597a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d138418-378d-4fc3-85ae-2006347340a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_community.vectorstores.faiss.FAISS'>\n"
     ]
    }
   ],
   "source": [
    "print(type(vectorstore))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a557cdfe-9045-430d-80c8-0ef323f1caee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_20400\\141266844.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 이전 대화 기억할 수 있는 메모리 생성\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# RAG 체인 구성\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    ChatOpenAI(),                  # OpenAI LLM\n",
    "    vectorstore.as_retriever(),   # FAISS 기반 검색기\n",
    "    memory=memory,                # 대화 메모리\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89d65e71-a3c0-45d7-965a-e4d2d90d0d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_20400\\49376575.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"question\": question})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "죄송합니다. LangChain에 대해 알려드릴 수 없습니다.\n"
     ]
    }
   ],
   "source": [
    "question = \"LangChain이 뭐야?\"\n",
    "result = qa_chain({\"question\": question})\n",
    "print(result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1798144f-300d-404d-bd9d-e96bcb4a5ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] OPENAI_API_KEY=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\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = vectorstore.similarity_search(\"LangChain이 뭐야?\")\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"[{i}] {r.page_content[:300]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b720b5c5-c9ca-46f5-9a30-c6ad2ce05d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'API 키는 보안 및 확인 목적으로 사용됩니다. API 키를 사용하여 사용자가 데이터에 엑세스하고 정보에 대한 요청을 '\n",
      "           '보낼 수 있습니다. 이렇게 하면 데이터에 대한 액세스를 제한하고 추적할 수 있습니다. API를 호출할 때 API 키를 '\n",
      "           '제공하여 인증 및 권한 부여를 할 수 있습니다.',\n",
      " 'chat_history': [HumanMessage(content='LangChain이 뭐야?', additional_kwargs={}, response_metadata={}),\n",
      "                  AIMessage(content='죄송합니다. LangChain에 대해 알려드릴 수 없습니다.', additional_kwargs={}, response_metadata={}),\n",
      "                  HumanMessage(content='이 문서에서 API 키는 어디에 사용된다고 나와 있어?', additional_kwargs={}, response_metadata={}),\n",
      "                  AIMessage(content='API 키는 보안 및 확인 목적으로 사용됩니다. API 키를 사용하여 사용자가 데이터에 엑세스하고 정보에 대한 요청을 보낼 수 있습니다. 이렇게 하면 데이터에 대한 액세스를 제한하고 추적할 수 있습니다. API를 호출할 때 API 키를 제공하여 인증 및 권한 부여를 할 수 있습니다.', additional_kwargs={}, response_metadata={})],\n",
      " 'question': '이 문서에서 API 키는 어디에 사용된다고 나와 있어?'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "question = \"이 문서에서 API 키는 어디에 사용된다고 나와 있어?\"\n",
    "result = qa_chain.invoke({\"question\": question})\n",
    "pprint(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00cb7383-3413-4406-a053-36da9bd7bac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "죄송합니다. LangChain에 대해 알고 있지만 문서를 어떻게 나누는지에 대한 구체적인 정보는 가지고 있지 않습니다. LangChain은 OpenAI에서 개발한 언어 처리 모델이며, 이 모델이 문서를 어떻게 처리하고 분할하는지에 대한 세부 내용을 알 수 있는 정보는 현재 제가 가지고 있지 않습니다. 추가적인 문의나 도움이 필요하시면 다시 문의해주시기 바랍니다. 감사합니다.\n"
     ]
    }
   ],
   "source": [
    "question = \"LangChain에서 문서를 어떻게 나누는지 알려줘\"\n",
    "result = qa_chain.invoke({\"question\": question})\n",
    "print(result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99fcad4b-c074-4236-ac5d-2c42bbf886da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "죄송합니다. 'RecursiveCharacterTextSplitter'에 대한 정보나 설명을 가지고 있지 않습니다.\n"
     ]
    }
   ],
   "source": [
    "question = \"이 문서에서 사용된 RecursiveCharacterTextSplitter가 문서를 나누는 방식은?\"\n",
    "result = qa_chain.invoke({\"question\": question})\n",
    "print(result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38431d41-b46c-4181-99ae-e5655ca3aa51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'LangChain에서 문서를 나누는 방식은 뭐야?', 'chat_history': [HumanMessage(content='LangChain이 뭐야?', additional_kwargs={}, response_metadata={}), AIMessage(content='죄송합니다. LangChain에 대해 알려드릴 수 없습니다.', additional_kwargs={}, response_metadata={}), HumanMessage(content='이 문서에서 API 키는 어디에 사용된다고 나와 있어?', additional_kwargs={}, response_metadata={}), AIMessage(content='API 키는 보안 및 확인 목적으로 사용됩니다. API 키를 사용하여 사용자가 데이터에 엑세스하고 정보에 대한 요청을 보낼 수 있습니다. 이렇게 하면 데이터에 대한 액세스를 제한하고 추적할 수 있습니다. API를 호출할 때 API 키를 제공하여 인증 및 권한 부여를 할 수 있습니다.', additional_kwargs={}, response_metadata={}), HumanMessage(content='LangChain에서 문서를 어떻게 나누는지 알려줘', additional_kwargs={}, response_metadata={}), AIMessage(content='죄송합니다. LangChain에 대해 알고 있지만 문서를 어떻게 나누는지에 대한 구체적인 정보는 가지고 있지 않습니다. LangChain은 OpenAI에서 개발한 언어 처리 모델이며, 이 모델이 문서를 어떻게 처리하고 분할하는지에 대한 세부 내용을 알 수 있는 정보는 현재 제가 가지고 있지 않습니다. 추가적인 문의나 도움이 필요하시면 다시 문의해주시기 바랍니다. 감사합니다.', additional_kwargs={}, response_metadata={}), HumanMessage(content='이 문서에서 사용된 RecursiveCharacterTextSplitter가 문서를 나누는 방식은?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"죄송합니다. 'RecursiveCharacterTextSplitter'에 대한 정보나 설명을 가지고 있지 않습니다.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='LangChain에서 문서를 나누는 방식은 뭐야?', additional_kwargs={}, response_metadata={}), AIMessage(content='죄송합니다, 현재는 LangChain이나 그 문서를 대해 제공하는 서비스에 대해 알지 못합니다.', additional_kwargs={}, response_metadata={})], 'answer': '죄송합니다, 현재는 LangChain이나 그 문서를 대해 제공하는 서비스에 대해 알지 못합니다.'}\n"
     ]
    }
   ],
   "source": [
    "question = \"LangChain에서 문서를 나누는 방식은 뭐야?\"\n",
    "result = qa_chain.invoke({\"question\": question})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c676fa85-0258-4ca6-8581-cebea1778c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] OPENAI_API_KEY=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\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vectorstore에 \"RecursiveCharacterTextSplitter\"와 관련된 문서가 있는지 확인\n",
    "docs = vectorstore.similarity_search(\"RecursiveCharacterTextSplitter\", k=2)\n",
    "\n",
    "for i, d in enumerate(docs):\n",
    "    print(f\"[{i+1}] {d.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "003de8fc-b6a0-46bc-bf36-7f5cadc0b053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] OPENAI_API_KEY=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\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vectorstore에 \"RecursiveCharacterTextSplitter\" 관련 내용이 잘 들어갔는지 검색\n",
    "docs = vectorstore.similarity_search(\"RecursiveCharacterTextSplitter\", k=2)\n",
    "\n",
    "for i, d in enumerate(docs):\n",
    "    print(f\"[{i+1}] {d.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61439444-e403-425a-884f-1b1ff2cd9f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] OPENAI_API_KEY=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\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 벡터스토어에 저장된 문서 중 \"RecursiveCharacterTextSplitter\"와 관련된 내용 검색\n",
    "docs = vectorstore.similarity_search(\"RecursiveCharacterTextSplitter\", k=2)\n",
    "\n",
    "for i, d in enumerate(docs):\n",
    "    print(f\"[{i+1}] {d.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fc87c8c-f13e-488e-8683-cde68a8765e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 개수: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"문서 개수: {len(docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62e85239-3291-4556-a438-cd797af454c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "OPENAI_API_KEY=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\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs[:5]):\n",
    "    print(f\"[{i+1}]\\n{doc.page_content}\\n{'-'*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dd2fd14-0734-4d7d-ab8a-aa6f2b8a2c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벡터 저장소 문서 수: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"벡터 저장소 문서 수: {len(vectorstore.similarity_search('', k=100))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3fe9ba4-f2dc-481b-ab39-6c77b4932042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 개수: 17\n",
      "[1]\n",
      "Introduction | 🦜️🔗 LangChain\n",
      "--------------------------------------------------------------------------------\n",
      "[2]\n",
      "Skip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1💬SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add\n",
      "--------------------------------------------------------------------------------\n",
      "[3]\n",
      "to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/docs/get_started/introduction\")\n",
    "docs = loader.load()\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"문서 개수: {len(docs)}\")\n",
    "for i, doc in enumerate(docs[:3]):\n",
    "    print(f\"[{i+1}]\\n{doc.page_content}\\n{'-'*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12d89d1b-ea2c-4b85-b815-cd7ac7a89771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    ChatOpenAI(),\n",
    "    vectorstore.as_retriever(),\n",
    "    memory=memory,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca83e221-4f4c-4dfa-b706-6e11494c9cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'LangChain에서 문서를 나누는 방식은 뭐야?', 'chat_history': [HumanMessage(content='LangChain에서 문서를 나누는 방식은 뭐야?', additional_kwargs={}, response_metadata={}), AIMessage(content='죄송합니다, 현재 LangChain에 대한 정보는 없습니다.', additional_kwargs={}, response_metadata={})], 'answer': '죄송합니다, 현재 LangChain에 대한 정보는 없습니다.'}\n"
     ]
    }
   ],
   "source": [
    "# 질문을 던져봅니다.\n",
    "question = \"LangChain에서 문서를 나누는 방식은 뭐야?\"\n",
    "result = qa_chain.invoke({\"question\": question})\n",
    "\n",
    "# 결과 출력\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d051bd07-4e88-4e08-aaba-d5cd560d8040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': '이 문서에서 OPENAI_API_KEY는 어떤 용도로 사용돼?', 'chat_history': [HumanMessage(content='LangChain에서 문서를 나누는 방식은 뭐야?', additional_kwargs={}, response_metadata={}), AIMessage(content='죄송합니다, 현재 LangChain에 대한 정보는 없습니다.', additional_kwargs={}, response_metadata={}), HumanMessage(content='이 문서에서 OPENAI_API_KEY는 어떤 용도로 사용돼?', additional_kwargs={}, response_metadata={}), AIMessage(content='OPENAI_API_KEY는 OpenAI의 API에 액세스할 때 사용되는 인증 키입니다. 이 키를 사용하여 OpenAI API에 요청을 보내고 서비스를 이용할 수 있습니다.', additional_kwargs={}, response_metadata={})], 'answer': 'OPENAI_API_KEY는 OpenAI의 API에 액세스할 때 사용되는 인증 키입니다. 이 키를 사용하여 OpenAI API에 요청을 보내고 서비스를 이용할 수 있습니다.'}\n"
     ]
    }
   ],
   "source": [
    "question = \"이 문서에서 OPENAI_API_KEY는 어떤 용도로 사용돼?\"\n",
    "result = qa_chain.invoke({\"question\": question})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6338ec0d-a475-4ef4-bf70-4187c48fe6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
